# Zero-shot Post-training Quantization Methods Impact LM Performance Differently

Model quantization allows us to reduce the memory needed to store neural network models while having a minimal impact on performance. However, the relationship between quantization level, quantization methodologies, and performance are not fully understood. Moreover, while most forms of quantization aim to produce a model that requires less memory, the optimization process of deriving these quantized models is memory intensive in and of itself. Thus, we focus on a number of post-training quantization methods and their impact on BERT's linguistic performance. Furthermore most work in the area has focused on inference speed, while our work explores the impacts of quantization on physical storage and model performance.  We find that BERT's performance minimally decreases for most GLUE subtasks after quantization. We also find notable differences among different quantization methodologies' impact on overall performance, and memory usage.
